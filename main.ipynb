{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "995d07be",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-04-30T12:35:08.829130Z",
     "iopub.status.busy": "2024-04-30T12:35:08.828674Z",
     "iopub.status.idle": "2024-04-30T12:35:16.485835Z",
     "shell.execute_reply": "2024-04-30T12:35:16.484485Z"
    },
    "papermill": {
     "duration": 7.669224,
     "end_time": "2024-04-30T12:35:16.489101",
     "exception": false,
     "start_time": "2024-04-30T12:35:08.819877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import torch\n",
    "\n",
    "import torchvision\n",
    "from tqdm import trange\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import datetime as dt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot   as plt\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data  import Dataset\n",
    "from torch.autograd    import Variable\n",
    "from torch.optim       import lr_scheduler\n",
    "\n",
    "from torch.utils.data  import Dataset, DataLoader, Subset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision       import transforms, datasets, models\n",
    "from os                import listdir, makedirs, getcwd, remove\n",
    "from os.path           import isfile, join, abspath, exists, isdir, expanduser\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5db195b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T12:35:16.505264Z",
     "iopub.status.busy": "2024-04-30T12:35:16.503830Z",
     "iopub.status.idle": "2024-04-30T12:35:16.511044Z",
     "shell.execute_reply": "2024-04-30T12:35:16.509676Z"
    },
    "papermill": {
     "duration": 0.01805,
     "end_time": "2024-04-30T12:35:16.513827",
     "exception": false,
     "start_time": "2024-04-30T12:35:16.495777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "origin_path = \"./dataset\"\n",
    "train_path = join(origin_path, \"train/train\")\n",
    "test_path = join(origin_path,\"test/test\")\n",
    "extraimage_path = join(origin_path, \"extraimages/extraimages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c47a78c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T12:35:16.529346Z",
     "iopub.status.busy": "2024-04-30T12:35:16.528835Z",
     "iopub.status.idle": "2024-04-30T12:35:16.538012Z",
     "shell.execute_reply": "2024-04-30T12:35:16.536597Z"
    },
    "papermill": {
     "duration": 0.020271,
     "end_time": "2024-04-30T12:35:16.540690",
     "exception": false,
     "start_time": "2024-04-30T12:35:16.520419",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/albumentations/augmentations/dropout/cutout.py:49: FutureWarning: Cutout has been deprecated. Please use CoarseDropout\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Transformations for both the training and testing data\n",
    "mean=[0.485, 0.456, 0.406]\n",
    "std=[0.229, 0.224, 0.225]\n",
    "\n",
    "# Do data transforms here, Try many others\n",
    "train_transforms = A.Compose(\n",
    "        [\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.Rotate(),\n",
    "            A.Resize(height=256, width=256, p=1),\n",
    "            A.Cutout(num_holes=8, max_h_size=32, max_w_size=32, fill_value=0, p=0.5),\n",
    "             A.ToFloat(max_value=255.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "test_transforms = A.Compose(\n",
    "        [\n",
    "            A.Resize(height=256, width=256, p=1),\n",
    "             A.ToFloat(max_value=255.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06829670",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T12:35:16.555768Z",
     "iopub.status.busy": "2024-04-30T12:35:16.555248Z",
     "iopub.status.idle": "2024-04-30T12:35:16.567146Z",
     "shell.execute_reply": "2024-04-30T12:35:16.566217Z"
    },
    "papermill": {
     "duration": 0.022529,
     "end_time": "2024-04-30T12:35:16.569691",
     "exception": false,
     "start_time": "2024-04-30T12:35:16.547162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CassavaDataset(Dataset):\n",
    "    def __init__(self, path, transform=None):\n",
    "        self.classes = os.listdir(path)\n",
    "        self.path = [f\"{path}/{className}\" for className in self.classes]\n",
    "        self.file_list = [glob.glob(f\"{x}/*\") for x in self.path]\n",
    "        self.transform = transform\n",
    "        self.classes_dict = {}\n",
    "\n",
    "        files = []\n",
    "        for i, className in enumerate(self.classes):\n",
    "            for fileName in self.file_list[i]:\n",
    "                self.classes_dict[i] = className\n",
    "                files.append([i, className, fileName])\n",
    "        self.file_list = files\n",
    "        files = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def get_image_filename(self, idx):\n",
    "        file_path = self.file_list[idx][2]\n",
    "        file_name = file_path.split(\"/\")[-1]\n",
    "        return file_name\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fileName = self.file_list[idx][2]\n",
    "        classCategory = self.file_list[idx][0]\n",
    "        im = Image.open(fileName)\n",
    "        if self.transform:\n",
    "            im = self.transform(image= np.asarray(im))[\"image\"]\n",
    "            \n",
    "        return im, classCategory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a755ef0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-30T12:35:16.586009Z",
     "iopub.status.busy": "2024-04-30T12:35:16.584612Z",
     "iopub.status.idle": "2024-04-30T12:35:17.035713Z",
     "shell.execute_reply": "2024-04-30T12:35:17.033499Z"
    },
    "papermill": {
     "duration": 0.461985,
     "end_time": "2024-04-30T12:35:17.038476",
     "exception": true,
     "start_time": "2024-04-30T12:35:16.576491",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = CassavaDataset(train_path, transform=train_transforms)\n",
    "test_data = CassavaDataset(test_path, transform=test_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a6127f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "BATCH_SIZE = 16\n",
    "# K-fold Cross Validation model evaluation\n",
    "data_loaders = []\n",
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(train_data)):\n",
    "    train_dataset = Subset(train_data, train_ids)\n",
    "    val_dataset = Subset(train_data, val_ids)\n",
    "\n",
    "    # # Creating PT data samplers and loaders:\n",
    "    # train_sampler = SubsetRandomSampler(train_ids)\n",
    "    # valid_sampler = SubsetRandomSampler(val_ids)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                                                 drop_last=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                                                 drop_last=True)\n",
    "\n",
    "    data_loaders.append((train_loader, valid_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "650200d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-29T11:50:49.291431Z",
     "iopub.status.busy": "2021-03-29T11:50:49.290391Z",
     "iopub.status.idle": "2021-03-29T11:50:49.293477Z",
     "shell.execute_reply": "2021-03-29T11:50:49.292843Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation_split = .2\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(train_data)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf3bb9bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-29T11:50:49.356639Z",
     "iopub.status.busy": "2021-03-29T11:50:49.355943Z",
     "iopub.status.idle": "2021-03-29T11:50:49.359522Z",
     "shell.execute_reply": "2021-03-29T11:50:49.360085Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d938e03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n"
     ]
    }
   ],
   "source": [
    "def make_model():\n",
    "    model = EfficientNet.from_pretrained('efficientnet-b4') \n",
    "    model._fc = nn.Linear(in_features=1792, out_features=5, bias=True)\n",
    "    return model\n",
    "\n",
    "model = make_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c92dfe6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-29T11:50:49.396116Z",
     "iopub.status.busy": "2021-03-29T11:50:49.395422Z",
     "iopub.status.idle": "2021-03-29T11:50:49.428700Z",
     "shell.execute_reply": "2021-03-29T11:50:49.428001Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Models \n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        # Block 1\n",
    "        self.conv1 = nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size = 2)\n",
    "\n",
    "        #Block 2\n",
    "        self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 3, stride = 2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        #Block 3\n",
    "        self.conv3 = nn.Conv2d(in_channels = 64, out_channels = 32, kernel_size = 3, stride = 2)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        # last fully-connected layer\n",
    "        self.fc = nn.Linear(32*3*3, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        x = self.maxpool1(self.relu1(self.conv1(input)))\n",
    "        x = self.maxpool2(self.relu2(self.conv2(x)))\n",
    "        x = self.maxpool3(self.relu3(self.conv3(x)))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# model = Classifier(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b362a09f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-29T11:50:49.458735Z",
     "iopub.status.busy": "2021-03-29T11:50:49.457972Z",
     "iopub.status.idle": "2021-03-29T11:50:49.461960Z",
     "shell.execute_reply": "2021-03-29T11:50:49.461357Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = lr_scheduler.OneCycleLR(\n",
    "                                    optimizer,\n",
    "                                    max_lr=0.001,\n",
    "                                    epochs=30,\n",
    "                                    steps_per_epoch=int(len(train_data) / BATCH_SIZE), \n",
    "                                    pct_start=0.1,\n",
    "                                    anneal_strategy='cos',\n",
    "                                    final_div_factor=10**5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f21abcb4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-29T11:50:49.496069Z",
     "iopub.status.busy": "2021-03-29T11:50:49.495310Z",
     "iopub.status.idle": "2021-03-29T11:50:49.499172Z",
     "shell.execute_reply": "2021-03-29T11:50:49.498453Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, scheduler, num_epochs):\n",
    "    \"\"\"Simple training loop for a PyTorch model.\"\"\" \n",
    "    \n",
    "    # Make sure model is in training mode.\n",
    "    model.train()\n",
    "    \n",
    "    # Move model to the device (CPU or MPS or GPU).\n",
    "    model.to(device)\n",
    "    \n",
    "    # Exponential moving average of the loss.\n",
    "    ema_loss = 0\n",
    "\n",
    "    print('----- Training Loop -----')\n",
    "    # Loop over epochs.\n",
    "    for epoch in trange(num_epochs):\n",
    "      for fold, (train_loader,valid_loader) in enumerate(data_loader):\n",
    "        correct = 0  # Reset correct for each fold\n",
    "        # Loop over data.\n",
    "        for _, (features, target) in enumerate(train_loader):\n",
    "              \n",
    "          # Forward pass.\n",
    "          output = model(features.to(device))\n",
    "          loss = criterion(output.to(device), target.to(device))\n",
    "\n",
    "\n",
    "          # Backward pass.\n",
    "          optimizer.zero_grad()\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          scheduler.step()\n",
    "\n",
    "        # Evaluate on the validation set\n",
    "        model.eval()\n",
    "        total_valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (features, target) in enumerate(valid_loader):\n",
    "                output = model(features.to(device))\n",
    "                loss = criterion(output.to(device), target.to(device))\n",
    "                # Get the label corresponding to the highest predicted probability.\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                # Count number of correct predictions.\n",
    "                correct += pred.cpu().eq(target.view_as(pred)).sum().item()\n",
    "                total_valid_loss += loss.item()\n",
    "        valid_loss_avg = total_valid_loss / len(valid_loader)\n",
    "        # NOTE: It is important to call .item() on the loss before summing.\n",
    "        if ema_loss is None:\n",
    "            ema_loss = loss.item()\n",
    "        else:\n",
    "            ema_loss += (loss.item() - ema_loss) * 0.01\n",
    "      \n",
    "        print(f'Epoch: {epoch}, Fold: {fold}, Train Loss: {ema_loss:.6f}, Valid Loss: {valid_loss_avg:.6f}')\n",
    "        # Print test accuracy.\n",
    "        percent = 100. * correct / len(valid_loader.dataset)  # Use valid_loader.dataset instead of data_loader.dataset\n",
    "        print(f'Validation accuracy: {correct} / {len(valid_loader.dataset)} ({percent:.0f}%)')\n",
    "        torch.save(model.state_dict(), 'model.ckpt')\n",
    "        # Reset model to training mode\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5101053",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-29T11:50:49.532699Z",
     "iopub.status.busy": "2021-03-29T11:50:49.531985Z",
     "iopub.status.idle": "2021-03-29T11:50:49.536002Z",
     "shell.execute_reply": "2021-03-29T11:50:49.535353Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(model, data_loader):\n",
    "    \"\"\"Measures the accuracy of a model on a data set.\"\"\" \n",
    "    # Make sure the model is in evaluation mode.\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    print('----- Model Evaluation -----')\n",
    "    # We do not need to maintain intermediate activations while testing.\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Loop over test data.\n",
    "        for images, target in data_loader:\n",
    "            # Forward pass.\n",
    "            output = model(images.to(device))\n",
    "            \n",
    "            # Get the label corresponding to the highest predicted probability.\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            # Count number of correct predictions.\n",
    "            correct += pred.cpu().eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    # Print test accuracy.\n",
    "    percent = 100. * correct / len(data_loader.dataset)\n",
    "    print(f'Validation accuracy: {correct} / {len(data_loader.dataset)} ({percent:.0f}%)')\n",
    "    torch.save(model.state_dict(), 'model.ckpt')\n",
    "    return percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d032bf01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-29T11:50:49.567361Z",
     "iopub.status.busy": "2021-03-29T11:50:49.566686Z",
     "iopub.status.idle": "2021-03-29T12:00:25.476410Z",
     "shell.execute_reply": "2021-03-29T12:00:25.475689Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Training Loop -----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Fold: 0, Train Loss: 0.006904, Valid Loss: 0.676838\n",
      "Validation accuracy: 836 / 1132 (74%)\n",
      "Epoch: 0, Fold: 1, Train Loss: 0.013317, Valid Loss: 0.637348\n",
      "Validation accuracy: 871 / 1131 (77%)\n",
      "Epoch: 0, Fold: 2, Train Loss: 0.016681, Valid Loss: 0.781420\n",
      "Validation accuracy: 865 / 1131 (76%)\n",
      "Epoch: 0, Fold: 3, Train Loss: 0.019841, Valid Loss: 0.597935\n",
      "Validation accuracy: 884 / 1131 (78%)\n",
      "Epoch: 0, Fold: 4, Train Loss: 0.029914, Valid Loss: 1.183779\n",
      "Validation accuracy: 722 / 1131 (64%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [17:37<1:10:28, 1057.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Fold: 0, Train Loss: 0.043478, Valid Loss: 0.493725\n",
      "Validation accuracy: 960 / 1132 (85%)\n",
      "Epoch: 1, Fold: 1, Train Loss: 0.045935, Valid Loss: 0.345560\n",
      "Validation accuracy: 992 / 1131 (88%)\n",
      "Epoch: 1, Fold: 2, Train Loss: 0.047316, Valid Loss: 0.439324\n",
      "Validation accuracy: 948 / 1131 (84%)\n",
      "Epoch: 1, Fold: 3, Train Loss: 0.050744, Valid Loss: 0.389708\n",
      "Validation accuracy: 978 / 1131 (86%)\n",
      "Epoch: 1, Fold: 4, Train Loss: 0.057589, Valid Loss: 0.348040\n",
      "Validation accuracy: 990 / 1131 (88%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [34:47<52:04, 1041.57s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Fold: 0, Train Loss: 0.060922, Valid Loss: 0.304472\n",
      "Validation accuracy: 1005 / 1132 (89%)\n",
      "Epoch: 2, Fold: 1, Train Loss: 0.072004, Valid Loss: 0.308994\n",
      "Validation accuracy: 998 / 1131 (88%)\n",
      "Epoch: 2, Fold: 2, Train Loss: 0.075355, Valid Loss: 0.314948\n",
      "Validation accuracy: 1002 / 1131 (89%)\n",
      "Epoch: 2, Fold: 3, Train Loss: 0.075918, Valid Loss: 0.227905\n",
      "Validation accuracy: 1047 / 1131 (93%)\n",
      "Epoch: 2, Fold: 4, Train Loss: 0.077142, Valid Loss: 0.219995\n",
      "Validation accuracy: 1035 / 1131 (92%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [51:57<34:32, 1036.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Fold: 0, Train Loss: 0.082688, Valid Loss: 0.225126\n",
      "Validation accuracy: 1032 / 1132 (91%)\n",
      "Epoch: 3, Fold: 1, Train Loss: 0.084420, Valid Loss: 0.201329\n",
      "Validation accuracy: 1043 / 1131 (92%)\n",
      "Epoch: 3, Fold: 2, Train Loss: 0.084486, Valid Loss: 0.322272\n",
      "Validation accuracy: 1009 / 1131 (89%)\n",
      "Epoch: 3, Fold: 3, Train Loss: 0.087249, Valid Loss: 0.303244\n",
      "Validation accuracy: 996 / 1131 (88%)\n",
      "Epoch: 3, Fold: 4, Train Loss: 0.087616, Valid Loss: 0.136828\n",
      "Validation accuracy: 1067 / 1131 (94%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [3:19:56<1:08:52, 4132.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Fold: 0, Train Loss: 0.087260, Valid Loss: 0.136417\n",
      "Validation accuracy: 1067 / 1132 (94%)\n",
      "Epoch: 4, Fold: 1, Train Loss: 0.087445, Valid Loss: 0.109469\n",
      "Validation accuracy: 1076 / 1131 (95%)\n",
      "Epoch: 4, Fold: 2, Train Loss: 0.090455, Valid Loss: 0.182552\n",
      "Validation accuracy: 1051 / 1131 (93%)\n",
      "Epoch: 4, Fold: 3, Train Loss: 0.091483, Valid Loss: 0.091418\n",
      "Validation accuracy: 1088 / 1131 (96%)\n",
      "Epoch: 4, Fold: 4, Train Loss: 0.091026, Valid Loss: 0.078240\n",
      "Validation accuracy: 1092 / 1131 (97%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [6:44:14<00:00, 4850.98s/it]  \n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "train(model, data_loaders, optimizer, scheduler, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "771cb14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for images, _ in data_loader:\n",
    "        # Forward pass.\n",
    "        output = model(images.to(device))\n",
    "        # Get the label corresponding to the highest predicted probability.\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        for p in pred:\n",
    "            pred_class = train_data.classes_dict[p.item()]\n",
    "            preds.append(pred_class)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e39642ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-29T12:00:25.513240Z",
     "iopub.status.busy": "2021-03-29T12:00:25.512426Z",
     "iopub.status.idle": "2021-03-29T12:00:25.515568Z",
     "shell.execute_reply": "2021-03-29T12:00:25.516089Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_submission_file(predictions):\n",
    "    mapped_preds = []\n",
    "    for idx in range(len(test_data)):\n",
    "        mapped_preds.append({'Category': predictions[idx], 'Id': test_data.get_image_filename(idx)})\n",
    "    pd.DataFrame(mapped_preds).to_csv(\"sample_submission_file.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(model, test_loader)\n",
    "# Make submission here\n",
    "generate_submission_file(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8400328,
     "sourceId": 77142,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 13.207064,
   "end_time": "2024-04-30T12:35:18.672226",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-30T12:35:05.465162",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
